package main

import (
	"fmt"
	"os"
	"sync"
	"time"
)

// å·¥ä½œæ± ç›¸å…³ç»“æ„

// å·¥ä½œæ± é…ç½®
type WorkerPoolConfig struct {
	MaxWorkers   int           `json:"max_workers"`   // æœ€å¤§å·¥ä½œåç¨‹æ•°
	QueueSize    int           `json:"queue_size"`    // é˜Ÿåˆ—å¤§å°
	BatchSize    int           `json:"batch_size"`    // æ‰¹å¤„ç†å¤§å°
	Timeout      time.Duration `json:"timeout"`       // è¶…æ—¶æ—¶é—´
	RetryCount   int           `json:"retry_count"`   // é‡è¯•æ¬¡æ•°
	BackoffDelay time.Duration `json:"backoff_delay"` // é€€é¿å»¶è¿Ÿ
	Enabled      bool          `json:"enabled"`       // æ˜¯å¦å¯ç”¨
}

// å¤„ç†ä»»åŠ¡
type ProcessingJob struct {
	ID        string                 `json:"id"`
	Lines     []string               `json:"lines"`
	Format    string                 `json:"format"`
	Priority  int                    `json:"priority"`
	CreatedAt time.Time              `json:"created_at"`
	Metadata  map[string]interface{} `json:"metadata"`
}

// å¤„ç†ç»“æœ
type ProcessingResult struct {
	JobID          string                 `json:"job_id"`
	ProcessedLines int                    `json:"processed_lines"`
	FilteredLines  int                    `json:"filtered_lines"`
	AlertedLines   int                    `json:"alerted_lines"`
	ErrorCount     int                    `json:"error_count"`
	ProcessingTime time.Duration          `json:"processing_time"`
	CreatedAt      time.Time              `json:"created_at"`
	Results        []LogAnalysis          `json:"results"`
	Errors         []string               `json:"errors"`
	Metadata       map[string]interface{} `json:"metadata"`
}

// å·¥ä½œæ± ç»Ÿè®¡
type WorkerPoolStats struct {
	TotalJobs     int64         `json:"total_jobs"`
	CompletedJobs int64         `json:"completed_jobs"`
	FailedJobs    int64         `json:"failed_jobs"`
	ActiveWorkers int           `json:"active_workers"`
	QueueLength   int           `json:"queue_length"`
	AverageTime   time.Duration `json:"average_time"`
	TotalLines    int64         `json:"total_lines"`
	ErrorRate     float64       `json:"error_rate"`
	Throughput    float64       `json:"throughput"` // æ¯ç§’å¤„ç†è¡Œæ•°
}

// å·¥ä½œæ± 
type WorkerPool struct {
	config     WorkerPoolConfig
	jobQueue   chan ProcessingJob
	resultChan chan ProcessingResult
	workerPool chan chan ProcessingJob
	workers    []*Worker
	quit       chan bool
	stats      WorkerPoolStats
	mutex      sync.RWMutex
	startTime  time.Time
}

// å·¥ä½œåç¨‹
type Worker struct {
	ID            int
	WorkerPool    chan chan ProcessingJob
	JobChannel    chan ProcessingJob
	Quit          chan bool
	WorkerPoolRef *WorkerPool
}

// å·¥ä½œæ± æ–¹æ³•

// åˆ›å»ºæ–°çš„å·¥ä½œæ± 
func NewWorkerPool(config WorkerPoolConfig) *WorkerPool {
	wp := &WorkerPool{
		config:     config,
		jobQueue:   make(chan ProcessingJob, config.QueueSize),
		resultChan: make(chan ProcessingResult, config.QueueSize),
		workerPool: make(chan chan ProcessingJob, config.MaxWorkers),
		workers:    make([]*Worker, 0, config.MaxWorkers),
		quit:       make(chan bool),
		startTime:  time.Now(),
	}

	// åˆ›å»ºå·¥ä½œåç¨‹
	for i := 0; i < config.MaxWorkers; i++ {
		worker := NewWorker(i, wp)
		wp.workers = append(wp.workers, worker)
		worker.Start()
	}

	// å¯åŠ¨è°ƒåº¦å™¨
	go wp.dispatch()

	return wp
}

// åˆ›å»ºæ–°çš„å·¥ä½œåç¨‹
func NewWorker(id int, wp *WorkerPool) *Worker {
	return &Worker{
		ID:            id,
		WorkerPool:    wp.workerPool,
		JobChannel:    make(chan ProcessingJob),
		Quit:          make(chan bool),
		WorkerPoolRef: wp,
	}
}

// å¯åŠ¨å·¥ä½œåç¨‹
func (w *Worker) Start() {
	go func() {
		for {
			// å°†å·¥ä½œåç¨‹çš„é€šé“æ³¨å†Œåˆ°å·¥ä½œæ± 
			w.WorkerPool <- w.JobChannel

			select {
			case job := <-w.JobChannel:
				// å¤„ç†ä»»åŠ¡
				w.processJob(job)
			case <-w.Quit:
				return
			}
		}
	}()
}

// åœæ­¢å·¥ä½œåç¨‹
func (w *Worker) Stop() {
	go func() {
		w.Quit <- true
	}()
}

// å¤„ç†ä»»åŠ¡
func (w *Worker) processJob(job ProcessingJob) {
	startTime := time.Now()

	// æ›´æ–°ç»Ÿè®¡
	w.WorkerPoolRef.mutex.Lock()
	w.WorkerPoolRef.stats.ActiveWorkers++
	w.WorkerPoolRef.mutex.Unlock()

	defer func() {
		w.WorkerPoolRef.mutex.Lock()
		w.WorkerPoolRef.stats.ActiveWorkers--
		w.WorkerPoolRef.mutex.Unlock()
	}()

	result := ProcessingResult{
		JobID:          job.ID,
		ProcessedLines: len(job.Lines),
		CreatedAt:      time.Now(),
		Results:        make([]LogAnalysis, 0),
		Errors:         make([]string, 0),
		Metadata:       make(map[string]interface{}),
	}

	// å¤„ç†æ¯ä¸€è¡Œæ—¥å¿—
	for _, line := range job.Lines {
		// æ£€æŸ¥ç¼“å­˜
		logHash := generateLogHash(line)
		if cached, found := cacheManager.GetAIAnalysis(logHash); found {
			// ä½¿ç”¨ç¼“å­˜ç»“æœ
			result.Results = append(result.Results, LogAnalysis{
				Line:       line,
				Important:  true,
				Reason:     cached.Result,
				Confidence: cached.Confidence,
			})
			result.FilteredLines++
			continue
		}

		// åº”ç”¨è§„åˆ™è¿‡æ»¤
		if globalConfig.LocalFilter && ruleEngine != nil {
			filterResult := ruleEngine.Filter(line)
			if filterResult.ShouldIgnore {
				continue
			}
			if filterResult.ShouldProcess {
				// éœ€è¦AIåˆ†æ
				analysis, err := analyzeLogLine(line, job.Format)
				if err != nil {
					result.ErrorCount++
					result.Errors = append(result.Errors, fmt.Sprintf("åˆ†æå¤±è´¥: %v", err))
					continue
				}

				// ç¼“å­˜ç»“æœ
				cacheResult := &AIAnalysisCache{
					LogHash:    logHash,
					Result:     analysis.Reason,
					Confidence: analysis.Confidence,
					Model:      globalConfig.Model,
					CreatedAt:  time.Now(),
					ExpiresAt:  time.Now().Add(globalConfig.Cache.AITTL),
				}
				cacheManager.SetAIAnalysis(logHash, cacheResult)

				result.Results = append(result.Results, *analysis)
				if analysis.Important {
					result.AlertedLines++
				}
			}
		} else {
			// ç›´æ¥AIåˆ†æ
			analysis, err := analyzeLogLine(line, job.Format)
			if err != nil {
				result.ErrorCount++
				result.Errors = append(result.Errors, fmt.Sprintf("åˆ†æå¤±è´¥: %v", err))
				continue
			}

			// ç¼“å­˜ç»“æœ
			cacheResult := &AIAnalysisCache{
				LogHash:    logHash,
				Result:     analysis.Reason,
				Confidence: analysis.Confidence,
				Model:      globalConfig.Model,
				CreatedAt:  time.Now(),
				ExpiresAt:  time.Now().Add(globalConfig.Cache.AITTL),
			}
			cacheManager.SetAIAnalysis(logHash, cacheResult)

			result.Results = append(result.Results, *analysis)
			if analysis.Important {
				result.AlertedLines++
			}
		}
	}

	result.ProcessingTime = time.Since(startTime)

	// æ›´æ–°ç»Ÿè®¡
	w.WorkerPoolRef.mutex.Lock()
	w.WorkerPoolRef.stats.CompletedJobs++
	w.WorkerPoolRef.stats.TotalLines += int64(result.ProcessedLines)
	w.WorkerPoolRef.mutex.Unlock()

	// å‘é€ç»“æœ
	w.WorkerPoolRef.resultChan <- result
}

// è°ƒåº¦å™¨
func (wp *WorkerPool) dispatch() {
	for {
		select {
		case job := <-wp.jobQueue:
			// è·å–å¯ç”¨çš„å·¥ä½œåç¨‹
			worker := <-wp.workerPool
			// åˆ†é…ä»»åŠ¡
			worker <- job

			// æ›´æ–°ç»Ÿè®¡
			wp.mutex.Lock()
			wp.stats.TotalJobs++
			wp.stats.QueueLength = len(wp.jobQueue)
			wp.mutex.Unlock()

		case <-wp.quit:
			// åœæ­¢æ‰€æœ‰å·¥ä½œåç¨‹
			for _, worker := range wp.workers {
				worker.Stop()
			}
			return
		}
	}
}

// æäº¤ä»»åŠ¡
func (wp *WorkerPool) SubmitJob(job ProcessingJob) error {
	if !wp.config.Enabled {
		return fmt.Errorf("å·¥ä½œæ± æœªå¯ç”¨")
	}

	select {
	case wp.jobQueue <- job:
		return nil
	default:
		return fmt.Errorf("å·¥ä½œé˜Ÿåˆ—å·²æ»¡")
	}
}

// è·å–ç»“æœ
func (wp *WorkerPool) GetResult() <-chan ProcessingResult {
	return wp.resultChan
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (wp *WorkerPool) GetStats() WorkerPoolStats {
	wp.mutex.RLock()
	defer wp.mutex.RUnlock()

	// è®¡ç®—ååé‡
	elapsed := time.Since(wp.startTime)
	if elapsed > 0 {
		wp.stats.Throughput = float64(wp.stats.TotalLines) / elapsed.Seconds()
	}

	// è®¡ç®—é”™è¯¯ç‡
	if wp.stats.TotalJobs > 0 {
		wp.stats.ErrorRate = float64(wp.stats.FailedJobs) / float64(wp.stats.TotalJobs) * 100
	}

	return wp.stats
}

// åœæ­¢å·¥ä½œæ± 
func (wp *WorkerPool) Stop() {
	close(wp.quit)
}

// å·¥ä½œæ± ç®¡ç†å‘½ä»¤å¤„ç†å‡½æ•°

// æ˜¾ç¤ºå·¥ä½œæ± ç»Ÿè®¡ä¿¡æ¯
func handleWorkerStats() {
	fmt.Println("ğŸ“Š å·¥ä½œæ± ç»Ÿè®¡ä¿¡æ¯:")
	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

	// åŠ è½½é…ç½®
	if err := loadConfig(); err != nil {
		fmt.Printf("âŒ é…ç½®åŠ è½½å¤±è´¥: %v\n", err)
		os.Exit(1)
	}

	stats := workerPool.GetStats()
	fmt.Printf("æ€»ä»»åŠ¡æ•°: %d\n", stats.TotalJobs)
	fmt.Printf("å®Œæˆä»»åŠ¡æ•°: %d\n", stats.CompletedJobs)
	fmt.Printf("å¤±è´¥ä»»åŠ¡æ•°: %d\n", stats.FailedJobs)
	fmt.Printf("æ´»è·ƒå·¥ä½œåç¨‹æ•°: %d\n", stats.ActiveWorkers)
	fmt.Printf("é˜Ÿåˆ—é•¿åº¦: %d\n", stats.QueueLength)
	fmt.Printf("å¹³å‡å¤„ç†æ—¶é—´: %v\n", stats.AverageTime)
	fmt.Printf("æ€»å¤„ç†è¡Œæ•°: %d\n", stats.TotalLines)
	fmt.Printf("é”™è¯¯ç‡: %.2f%%\n", stats.ErrorRate)
	fmt.Printf("ååé‡: %.2f è¡Œ/ç§’\n", stats.Throughput)

	// æ˜¾ç¤ºé…ç½®ä¿¡æ¯
	fmt.Println("\nå·¥ä½œæ± é…ç½®:")
	fmt.Printf("  æœ€å¤§å·¥ä½œåç¨‹æ•°: %d\n", globalConfig.WorkerPool.MaxWorkers)
	fmt.Printf("  é˜Ÿåˆ—å¤§å°: %d\n", globalConfig.WorkerPool.QueueSize)
	fmt.Printf("  æ‰¹å¤„ç†å¤§å°: %d\n", globalConfig.WorkerPool.BatchSize)
	fmt.Printf("  è¶…æ—¶æ—¶é—´: %v\n", globalConfig.WorkerPool.Timeout)
	fmt.Printf("  é‡è¯•æ¬¡æ•°: %d\n", globalConfig.WorkerPool.RetryCount)
	fmt.Printf("  é€€é¿å»¶è¿Ÿ: %v\n", globalConfig.WorkerPool.BackoffDelay)
	fmt.Printf("  å¯ç”¨çŠ¶æ€: %t\n", globalConfig.WorkerPool.Enabled)
}

// æµ‹è¯•å·¥ä½œæ± åŠŸèƒ½
func handleWorkerTest() {
	fmt.Println("ğŸ§ª æµ‹è¯•å·¥ä½œæ± åŠŸèƒ½...")
	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

	// åŠ è½½é…ç½®
	if err := loadConfig(); err != nil {
		fmt.Printf("âŒ é…ç½®åŠ è½½å¤±è´¥: %v\n", err)
		os.Exit(1)
	}

	// åˆ›å»ºæµ‹è¯•ä»»åŠ¡
	testLines := []string{
		"2024-01-01 10:00:00 [ERROR] Database connection failed",
		"2024-01-01 10:00:01 [INFO] User login successful",
		"2024-01-01 10:00:02 [WARN] High memory usage detected",
		"2024-01-01 10:00:03 [DEBUG] Processing request",
		"2024-01-01 10:00:04 [ERROR] File not found",
	}

	job := ProcessingJob{
		ID:        "test_job_1",
		Lines:     testLines,
		Format:    "java",
		Priority:  1,
		CreatedAt: time.Now(),
		Metadata: map[string]interface{}{
			"test": true,
		},
	}

	fmt.Println("1. æäº¤æµ‹è¯•ä»»åŠ¡...")
	if err := workerPool.SubmitJob(job); err != nil {
		fmt.Printf("   âŒ ä»»åŠ¡æäº¤å¤±è´¥: %v\n", err)
		return
	}
	fmt.Println("   âœ… ä»»åŠ¡æäº¤æˆåŠŸ")

	// ç­‰å¾…ç»“æœ
	fmt.Println("2. ç­‰å¾…å¤„ç†ç»“æœ...")
	timeout := time.After(30 * time.Second)

	select {
	case result := <-workerPool.GetResult():
		fmt.Printf("   âœ… ä»»åŠ¡å¤„ç†å®Œæˆ: %s\n", result.JobID)
		fmt.Printf("   å¤„ç†è¡Œæ•°: %d\n", result.ProcessedLines)
		fmt.Printf("   è¿‡æ»¤è¡Œæ•°: %d\n", result.FilteredLines)
		fmt.Printf("   å‘Šè­¦è¡Œæ•°: %d\n", result.AlertedLines)
		fmt.Printf("   é”™è¯¯æ•°: %d\n", result.ErrorCount)
		fmt.Printf("   å¤„ç†æ—¶é—´: %v\n", result.ProcessingTime)
		fmt.Printf("   ç»“æœæ•°: %d\n", len(result.Results))

		if len(result.Errors) > 0 {
			fmt.Println("   é”™è¯¯è¯¦æƒ…:")
			for i, err := range result.Errors {
				fmt.Printf("     %d. %s\n", i+1, err)
			}
		}

	case <-timeout:
		fmt.Println("   âŒ ä»»åŠ¡å¤„ç†è¶…æ—¶")
		return
	}

	// æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
	fmt.Println("\næœ€ç»ˆå·¥ä½œæ± ç»Ÿè®¡:")
	stats := workerPool.GetStats()
	fmt.Printf("  æ€»ä»»åŠ¡æ•°: %d\n", stats.TotalJobs)
	fmt.Printf("  å®Œæˆä»»åŠ¡æ•°: %d\n", stats.CompletedJobs)
	fmt.Printf("  ååé‡: %.2f è¡Œ/ç§’\n", stats.Throughput)

	fmt.Println("\nâœ… å·¥ä½œæ± åŠŸèƒ½æµ‹è¯•å®Œæˆ")
}

// åˆ›å»ºä»»åŠ¡è°ƒåº¦å™¨
func NewTaskScheduler(workers []*Worker, loadBalancer *LoadBalancer) *TaskScheduler {
	return &TaskScheduler{
		priorityQueue: NewPriorityQueue(),
		workers:       workers,
		loadBalancer:  loadBalancer,
	}
}

// æäº¤ä»»åŠ¡
func (ts *TaskScheduler) SubmitTask(job ProcessingJob, priority TaskPriority) error {
	ts.mutex.Lock()
	defer ts.mutex.Unlock()

	// æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å·¥ä½œåç¨‹
	if len(ts.workers) == 0 {
		return fmt.Errorf("æ²¡æœ‰å¯ç”¨çš„å·¥ä½œåç¨‹")
	}

	// æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
	ts.priorityQueue.AddJob(job, priority)

	// å°è¯•ç«‹å³åˆ†é…ä»»åŠ¡
	ts.tryAssignTask()

	return nil
}

// å°è¯•åˆ†é…ä»»åŠ¡
func (ts *TaskScheduler) tryAssignTask() {
	// è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡
	job := ts.priorityQueue.GetNextJob()
	if job == nil {
		return
	}

	// é€‰æ‹©å·¥ä½œåç¨‹
	worker := ts.loadBalancer.SelectWorker()
	if worker == nil {
		// æ²¡æœ‰å¯ç”¨å·¥ä½œåç¨‹ï¼Œå°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—
		ts.priorityQueue.AddJob(*job, ts.priorityQueue.priorities[job.ID])
		return
	}

	// åˆ†é…ä»»åŠ¡
	select {
	case worker.JobChannel <- *job:
		// ä»»åŠ¡åˆ†é…æˆåŠŸ
		ts.stats.TotalJobs++
	default:
		// å·¥ä½œåç¨‹å¿™ï¼Œå°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—
		ts.priorityQueue.AddJob(*job, ts.priorityQueue.priorities[job.ID])
	}
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (ts *TaskScheduler) GetStats() ConcurrencyStats {
	ts.mutex.RLock()
	defer ts.mutex.RUnlock()

	return ts.stats
}
